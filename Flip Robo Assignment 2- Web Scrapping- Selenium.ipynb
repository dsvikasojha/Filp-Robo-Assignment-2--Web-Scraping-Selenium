{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the\n",
    "    job-title, job-location, company_name,experience_required. You have to scrape first 10 jobs data.\n",
    "    \n",
    "        1. first get the webpage https://www.naukri.com/\n",
    "        2. Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore” \n",
    "           in “enter the location” field.\n",
    "        3. Then click the search button.\n",
    "        4. Then scrape the data for the first 10 jobs results you get.\n",
    "        5. Finally create a dataframe of the scraped data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Title                             Company Name Experience  \\\n",
      "0  Data Analyst                             RedLock, Inc    0-0 Yrs   \n",
      "1  Data Analyst                    Super India Tech Mark    0-2 Yrs   \n",
      "2  Data Analyst                        tech mahindra ltd    4-8 Yrs   \n",
      "3  Data Analyst     CONDUENT BUSINESS SERVICES INDIA LLP    1-2 Yrs   \n",
      "4  Data Analyst  GlaxoSmithKline Pharmaceuticals Limited    2-7 Yrs   \n",
      "5  Data Analyst                 Myntra Designs Pvt. Ltd.    3-6 Yrs   \n",
      "6  Data Analyst                 Myntra Designs Pvt. Ltd.    3-8 Yrs   \n",
      "7  Data Analyst                 Myntra Designs Pvt. Ltd.    4-8 Yrs   \n",
      "8  Data Analyst                 Myntra Designs Pvt. Ltd.    4-8 Yrs   \n",
      "9  Data Analyst            WEIWO Communication Pvt. Ltd.    5-8 Yrs   \n",
      "\n",
      "                         Location  \n",
      "0             Bangalore/Bengaluru  \n",
      "1  Bangalore/Bengaluru(Devalapur)  \n",
      "2             Bangalore/Bengaluru  \n",
      "3             Bangalore/Bengaluru  \n",
      "4             Bangalore/Bengaluru  \n",
      "5             Bangalore/Bengaluru  \n",
      "6             Bangalore/Bengaluru  \n",
      "7             Bangalore/Bengaluru  \n",
      "8             Bangalore/Bengaluru  \n",
      "9     Bangalore/Bengaluru(Ulsoor)  \n"
     ]
    }
   ],
   "source": [
    "# Creating open Lists to save scraped data to scrape\n",
    "job_title = []\n",
    "comp_name = []\n",
    "location_list = []\n",
    "exp_req = []\n",
    "\n",
    "\n",
    "def naukri_da():    # Defining function naukri_da\n",
    "    driver = webdriver.Chrome(\"C://chromedriver.exe\") # Loading Chrome Driver\n",
    "    driver.get(\"https://www.naukri.com/\") # Website on which we have to scrape the data\n",
    "    \n",
    "    # Finding search input tab xpath and entering Data Analyst in it\n",
    "    search_job = driver.find_element_by_id(\"qsb-keyword-sugg\").send_keys(\"Data Analyst\")     \n",
    "    \n",
    "    # Searching Location input xpath and entering location Bangalore. \n",
    "    job_loc = driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\").send_keys(\"Bangalore\")\n",
    "    \n",
    "    # Finding search button xpath and clicking it to search \n",
    "    search_btn = driver.find_element_by_xpath(\"//div[@class='search-btn']/button\").click()\n",
    "    \n",
    "    time.sleep(3)   # giving time so that page can load fully\n",
    "    \n",
    "    # Finding title xpath and appending it\n",
    "    Tit = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "    for i in Tit[0:10]:\n",
    "        job_title.append(i.text)\n",
    "    \n",
    "    # Finding company name xpath and appending it\n",
    "    Com = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "    for j in Com[0:10]:\n",
    "        comp_name.append(j.text)\n",
    "    \n",
    "    # Finding location xpath and appending it\n",
    "    Loc = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "    for k in Loc[0:10]:\n",
    "        location_list.append(k.text)\n",
    "    \n",
    "    # Finding Experience Required xpath and appending it\n",
    "    Exp = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']\")\n",
    "    for l in Exp[0:10]:\n",
    "        exp_req.append(l.text)\n",
    "        \n",
    "       \n",
    "\n",
    "    jobs = pd.DataFrame({}) # Creating a dataframe and saving scraped data.\n",
    "    jobs['Title'] = job_title[0:10]\n",
    "    jobs['Company Name'] = comp_name[0:10]\n",
    "    jobs['Experience'] = exp_req[0:10]\n",
    "    jobs['Location'] = location_list[0:10]\n",
    "    print(jobs)\n",
    "\n",
    "# Calling Function\n",
    "naukri_da()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape\n",
    "    the job-title, job-location,company_name, full job-description. You have to scrape first 10 jobs data.\n",
    "    \n",
    "            1. first get the webpage https://www.naukri.com/\n",
    "            2. Enter “Data Scientist” in “Skill,Designations,Companies” field and enter \n",
    "               “Bangalore” in “enter the location” field.\n",
    "            3. Then click the search button.\n",
    "            4. Then scrape the data for the first 10 jobs results you get.\n",
    "            5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0                                     Data Scientist   \n",
      "1  Opportunity For Data Scientist Internship - Be...   \n",
      "2                            Data Scientist/ Analyst   \n",
      "3                  Data Scientist - Machine Learning   \n",
      "4     Data Scientist || Data Analyst || Data science   \n",
      "5             DBCG IND - GAMMA Senior Data Scientist   \n",
      "6               Data Scientist/Senior Data Scientist   \n",
      "7  Senior Data Scientist | CES IT LTD | CMMI Level 5   \n",
      "8           Associate Data Scientist - CRM & Loyalty   \n",
      "9              Data Scientist/Data Analyst-immediate   \n",
      "\n",
      "                                 Company Name             Location  \\\n",
      "0       CronJ IT Technologies Private Limited  Bangalore/Bengaluru   \n",
      "1                                              Bangalore/Bengaluru   \n",
      "2                      Corner Stone Solutions  Bangalore/Bengaluru   \n",
      "3                                              Bangalore/Bengaluru   \n",
      "4             Becton Dickinson India Pvt. Ltd          Navi Mumbai   \n",
      "5                                              Bangalore/Bengaluru   \n",
      "6                                 AugmatrixGo               Mumbai   \n",
      "7                                                        New Delhi   \n",
      "8  Inspiration Manpower Consultancy Pvt. Ltd.              Chennai   \n",
      "9                                              Bangalore/Bengaluru   \n",
      "\n",
      "                                         Description  \n",
      "0  Job description Responsibilities and Duties Cr...  \n",
      "1  Job description Location - Bangalore / Bengalu...  \n",
      "2  Job description Roles and Responsibilities ob ...  \n",
      "3  Job description Roles and Responsibilities   -...  \n",
      "4  Job description Job description Job Summary an...  \n",
      "5  Job description     What Youll Do  We re looki...  \n",
      "6  Job description  About Ganit Inc  Founded by s...  \n",
      "7  Job description Roles and Responsibilities  Mu...  \n",
      "8  Job description The Role General Position Defi...  \n",
      "9  Job description Job description Dear Candidate...  \n"
     ]
    }
   ],
   "source": [
    "# Creating open list to save scraped data\n",
    "job_title = []\n",
    "comp_name = []\n",
    "location_list = []\n",
    "job_description = []\n",
    "url_list = []\n",
    "\n",
    "\n",
    "def naukri_ds(): # Defining Function naukri_ds\n",
    "    driver = webdriver.Chrome(r\"C://chromedriver.exe\") # Loading Chrome Driver\n",
    "    driver.get(\"https://www.naukri.com/\") # Website in which we have to scrape data\n",
    "    \n",
    "    # Finding search input tab xpath and entering Data Scientist in it\n",
    "    search_job = driver.find_element_by_id(\"qsb-keyword-sugg\").send_keys(\"Data Scientist\")\n",
    "    \n",
    "    # Searching Location input xpath and entering location Bangalore. \n",
    "    job_loc = driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\").send_keys(\"Bangalore\")\n",
    "    \n",
    "    # Finding search button xpath and clicking it to search \n",
    "    search_btn = driver.find_element_by_xpath(\"//div[@class='search-btn']/button\").click() \n",
    "     \n",
    "    time.sleep(3)# giving time so that page can load fully\n",
    "    \n",
    "    # loop for searching href link to scrape by xpath and appending it to list\n",
    "    urls = driver.find_elements_by_xpath(\"//a[@class = 'title fw500 ellipsis']\")\n",
    "    for i in urls:\n",
    "        link = i.get_attribute('href')\n",
    "        url_list.append(link)\n",
    "    \n",
    "    time.sleep(3)\n",
    "    for x in url_list[0:15]: # Loop for extracting data from each href\n",
    "        driver.get(x)\n",
    "        time.sleep(5)\n",
    "       \n",
    "        # Searching job title by xpath from link\n",
    "        for i in driver.find_elements_by_xpath(\"//h1[@class = 'jd-header-title']\"):\n",
    "            job_title.append(i.text)\n",
    "        \n",
    "        # Searching company name by xpath from link\n",
    "        for j in driver.find_elements_by_xpath(\"//a[@class='pad-rt-8'or @class = 'subTitle ellipsis fleft']\"):\n",
    "            comp_name.append(j.text)\n",
    "        \n",
    "        # Searching location by xpath from link\n",
    "        for k in driver.find_elements_by_xpath(\"//span[@class='location ']/a\"):\n",
    "            location_list.append(k.text)\n",
    "        \n",
    "        # Searching job description by xpath from link\n",
    "        for l in driver.find_elements_by_xpath(\"//section[@class='job-desc']\"):\n",
    "            job_description.append(l.text.replace('\\n',' '))\n",
    "    \n",
    "    \n",
    "    jobs = pd.DataFrame({}) # Creating and saving data into dataframe\n",
    "    jobs['Title'] = job_title[0:10]\n",
    "    jobs['Company Name'] = comp_name[0:10]\n",
    "    jobs['Location'] = location_list[0:10]\n",
    "    jobs['Description'] = job_description[0:10]\n",
    "    print(jobs)\n",
    "    \n",
    "# Calling Function\n",
    "naukri_ds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage.\n",
    "    \n",
    "    The task will be done as shown in the below steps:\n",
    "        1. first get the webpage https://www.naukri.com/\n",
    "        2. Enter “Data Scientist” in “Skill,Designations,Companies” field.\n",
    "        3. Then click the search button.\n",
    "        4. Then apply the location filter and salary filter by checking the respective boxes.Then scrape the\n",
    "           data for the first 10 jobs results you get.\n",
    "        5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0                           Developer - Data Science   \n",
      "1  Data Scientist/Data Analyst - Python/Machine L...   \n",
      "2                                     Data Scientist   \n",
      "3         Data Scientist - Python & Machine Learning   \n",
      "4         Data Scientist - Python & Machine Learning   \n",
      "5  Data Scientist - Python / Machine Learning / T...   \n",
      "6         Data Scientist - Python & Machine Learning   \n",
      "7                          Hiring For Data Scientist   \n",
      "8  Required- Data Scientist (NLP)-Axis Bank - 6 m...   \n",
      "9  Data Scientist - Python / Machine Learning / T...   \n",
      "\n",
      "                                Company Name Experience  \\\n",
      "0          ICL Systems India Private Limited    3-5 Yrs   \n",
      "1                             Change leaders   5-10 Yrs   \n",
      "2                           Amity University    6-8 Yrs   \n",
      "3                        FUTURES AND CAREERS    2-7 Yrs   \n",
      "4                        FUTURES AND CAREERS    2-7 Yrs   \n",
      "5                        FUTURES AND CAREERS    3-8 Yrs   \n",
      "6                        FUTURES AND CAREERS    2-7 Yrs   \n",
      "7  Max Bupa Health Insurance Company Limited    1-6 Yrs   \n",
      "8                          Axis Bank Limited    4-9 Yrs   \n",
      "9                        FUTURES AND CAREERS    3-8 Yrs   \n",
      "\n",
      "                                            Location  \n",
      "0                                        Delhi / NCR  \n",
      "1                                  Mumbai, Ghaziabad  \n",
      "2                  Ghaziabad, Faridabad, Delhi / NCR  \n",
      "3  Hyderabad/Secunderabad, Pune, Bangalore/Bengal...  \n",
      "4  Hyderabad/Secunderabad, Pune, Chennai, Bangalo...  \n",
      "5  Mumbai, Hyderabad/Secunderabad, Bangalore/Beng...  \n",
      "6  Hyderabad/Secunderabad, Bangalore/Bengaluru, D...  \n",
      "7                      Gurgaon/Gurugram, Delhi / NCR  \n",
      "8  Kolkata, New Delhi, Hyderabad/Secunderabad, Pu...  \n",
      "9  Hyderabad/Secunderabad, Bangalore/Bengaluru, D...  \n"
     ]
    }
   ],
   "source": [
    "# Creating open list to save scrape data\n",
    "job_title = []\n",
    "comp_name = []\n",
    "location_list = []\n",
    "exp_req = []\n",
    "\n",
    "\n",
    "def naukri_ds_1(): # Defining function naukri_ds_1\n",
    "    driver = webdriver.Chrome(r\"C://chromedriver.exe\") # Loading chrome driver\n",
    "    driver.get(\"https://www.naukri.com/\") # website from which we have to scrape the data\n",
    "    \n",
    "    # Searching by xpath of job search and sending Data Scientist to input\n",
    "    search_job = driver.find_element_by_id(\"qsb-keyword-sugg\").send_keys(\"Data Scientist\")\n",
    "    \n",
    "    # Clicking on search button to begin process\n",
    "    driver.find_element_by_xpath(\"//div[@class='search-btn']/button\").click()\n",
    "    \n",
    "    time.sleep(3) \n",
    "    \n",
    "    # Filtering job location of Delhi/NCR\n",
    "    for m in driver.find_elements_by_xpath(\"//div[@class='mt-8 chckBoxCont']/label/p/span\"):\n",
    "        if m.text == 'Delhi / NCR':\n",
    "            m.click()\n",
    "            break    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Filtering salary '3-6 Lakhs'\n",
    "    for n in driver.find_elements_by_xpath(\"//div[@class='mt-8 chckBoxCont']/label/p/span\"):\n",
    "        if n.text == '3-6 Lakhs':\n",
    "            n.click()\n",
    "            break\n",
    "    \n",
    "    time.sleep(3)\n",
    "    # Finding Job Title by xpath and appending it to list \n",
    "    Tit = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "    for i in Tit[0:10]:\n",
    "        job_title.append(i.text)\n",
    "    \n",
    "    # Finding Company Name by xpath and appending it to list\n",
    "    Comp = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "    for j in Comp[0:10]:\n",
    "        comp_name.append(j.text)\n",
    "    \n",
    "    # Finding job location by xpath and appending it to list\n",
    "    Loc = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "    for k in Loc[0:10]:\n",
    "        location_list.append(k.text)\n",
    "    \n",
    "    # Finding required experience by xpath and appending it to list\n",
    "    Exp = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']\")\n",
    "    for l in Exp[0:10]:\n",
    "        exp_req.append(l.text)\n",
    "        \n",
    "    jobs = pd.DataFrame({}) # Creating and saving scraped data into dataframe\n",
    "    jobs['Title'] = job_title[0:10]\n",
    "    jobs['Company Name'] = comp_name[0:10]\n",
    "    jobs['Experience'] = exp_req[0:10]\n",
    "    jobs['Location'] = location_list[0:10]\n",
    "    print(jobs)\n",
    "\n",
    "# Calling Function\n",
    "naukri_ds_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. \n",
    "    You have to scrape company_name, No. of days ago when job was posted, Rating of the company.This task will be done\n",
    "    in following steps:\n",
    "            1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "            2. Enter “Data Scientist” in “Job Title,Keyword,Company” field and enter “Noida” \n",
    "               in “location” field.\n",
    "            3. Then click the search button.\n",
    "            4. Then scrape the data for the first 10 jobs results you get in the above shown \n",
    "               page.\n",
    "            5. Finally create a dataframe of the scraped data.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Company Name Days Posted(ago) Ratings\n",
      "0     Unyscape Infocom Pvt. Ltd             30d+     4.1\n",
      "1                Biz2Credit Inc             30d+     3.8\n",
      "2                      Techlive             30d+     5.0\n",
      "3                         Adobe               8d     4.4\n",
      "4                       Genpact               4d     3.8\n",
      "5  Salasar New Age Technologies             30d+     5.0\n",
      "6                       Asquero               9d     4.2\n",
      "7  Salasar New Age Technologies             30d+     4.4\n",
      "8                    MasterCard               1d     3.6\n",
      "9                     Microsoft             30d+     5.0\n"
     ]
    }
   ],
   "source": [
    "# Creating open list to save scraped data\n",
    "days_ago = []\n",
    "comp_name = []\n",
    "ratings = []\n",
    "\n",
    "\n",
    "def glassdoor(): # defining function glassdoor\n",
    "    driver = webdriver.Chrome(r\"C://chromedriver.exe\") # Loading chrome driver\n",
    "    driver.get(\"https://www.glassdoor.co.in/Salaries/index.htm\") # Website from which we will scraped data\n",
    "    \n",
    "    # Selecting search job tab by xpath\n",
    "    jobs = driver.find_element_by_xpath('//*[@id=\"SrchHero\"]/div/div[1]/div[1]/div/div/div/ul/li[1]').click()\n",
    "        \n",
    "    # finding search job tab by xpath and sending keyword\n",
    "    search_job = driver.find_element_by_xpath(\"//input[@id = 'KeywordSearch']\").send_keys(\"Data Scientist\")\n",
    "    \n",
    "    job_loc = driver.find_element_by_xpath(\"//input[@class='loc']\").clear()\n",
    "    driver.find_element_by_xpath(\"//input[@class='loc']\").send_keys(\"Noida\")\n",
    "    \n",
    "        \n",
    "        \n",
    "    # Clicking on search tab   \n",
    "    search_btn = driver.find_element_by_xpath(\"//button[@id='HeroSearchButton']\")\n",
    "    search_btn.click()\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Searching for company name by xpath\n",
    "    Comp = driver.find_elements_by_xpath(\"//a[@class=' css-l2wjgv e1n63ojh0 jobLink']/span\")\n",
    "    for i in Comp:\n",
    "        comp_name.append(i.text)\n",
    "    \n",
    "    # Searching for job posted date by xpath \n",
    "    Days = driver.find_elements_by_xpath(\"//div[@class='d-flex align-items-end pl-std css-mi55ob' or @class = 'd-flex align-items-end pl-std css-mi55ob']\")\n",
    "    for j in Days:\n",
    "        days_ago.append(j.text)\n",
    "    \n",
    "    # Searching for Ratingsby xpath\n",
    "    Rat = driver.find_elements_by_xpath(\"//span[@class='css-19pjha7 e1cjmv6j1']\")\n",
    "    for k in Rat:\n",
    "        ratings.append(k.text)\n",
    "    \n",
    "        \n",
    "       \n",
    "\n",
    "    jobs = pd.DataFrame({}) # Creating database of scraped results\n",
    "    jobs['Company Name'] = comp_name[0:10]\n",
    "    jobs['Days Posted(ago)'] = days_ago[0:10]\n",
    "    jobs['Ratings'] = ratings[0:10]\n",
    "    print(jobs)\n",
    "\n",
    "# Calling Function\n",
    "glassdoor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location.You have to\n",
    "    scrape Company name, Number of salaries, Average salary, Min salary, Max Salary.The above task will be, done as\n",
    "    shown in the below steps:\n",
    "            1. first get the webpage https://www.glassdoor.co.in/Salaries/index.htm\n",
    "            2. Enter “Data Scientist” in Job title field and “Noida” in location field.\n",
    "            3. Click the search button.\n",
    "            4. Scrape data for first 10 companies. Scrape the min salary, max salary, company name, Average salary\n",
    "               and rating of the company.\n",
    "            5.Store the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Company Name Total Salaries  Average Salary Min and Max Salary\n",
      "0  Tata Consultancy Services    14 salaries   ₹ 6,01,000/yr      ₹336L ₹1,080L\n",
      "1                  Accenture    14 salaries  ₹ 11,51,207/yr      ₹579L ₹2,222L\n",
      "2                  Delhivery    14 salaries  ₹ 12,34,207/yr     ₹452L ₹11,669L\n",
      "3                        IBM    13 salaries   ₹ 7,63,825/yr      ₹589L ₹2,741L\n",
      "4         Ericsson-Worldwide    12 salaries   ₹ 7,32,209/yr      ₹350L ₹1,619L\n",
      "5         UnitedHealth Group    10 salaries  ₹ 13,88,910/yr    ₹1,050L ₹1,500L\n",
      "6         Valiance Solutions     9 salaries   ₹ 8,18,515/yr      ₹504L ₹1,471L\n",
      "7                 Innovaccer     8 salaries  ₹ 12,01,403/yr      ₹623L ₹1,702L\n",
      "8              ZS Associates     7 salaries  ₹ 10,00,000/yr      ₹203L ₹1,817L\n",
      "9                EXL Service     7 salaries  ₹ 11,90,000/yr      ₹578L ₹1,500L\n"
     ]
    }
   ],
   "source": [
    "# Creating open list to store scraped data\n",
    "salary_no = []\n",
    "comp_name = []\n",
    "avg_sal = []\n",
    "min_sal = []\n",
    "\n",
    "def glassdoor_salary(): # defining Function\n",
    "    driver = webdriver.Chrome(r\"C://chromedriver.exe\") # Loading Chrome driver\n",
    "    driver.get(\"https://www.glassdoor.co.in/Salaries/index.htm\") # Website from which data to be scraped\n",
    "    \n",
    "    # Searching job input tab and sending keyword Data Scientist\n",
    "    search_job = driver.find_element_by_xpath(\"//input[@id = 'KeywordSearch']\").send_keys(\"Data Scientist\")\n",
    "    \n",
    "    # Searching job location tab and sending keyword Noida\n",
    "    job_loc = driver.find_element_by_xpath(\"//input[@class='loc']\").clear()\n",
    "    driver.find_element_by_xpath(\"//input[@class='loc']\").send_keys(\"Noida\")  \n",
    "    \n",
    "    # Clicking on search button\n",
    "    search_btn = driver.find_element_by_xpath(\"//button[@id='HeroSearchButton']\").click()\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Scraping comp_name data\n",
    "    Comp = driver.find_elements_by_xpath(\"//p[@class='m-0 ']\")\n",
    "    for i in Comp:\n",
    "        comp_name.append(i.text)\n",
    "    \n",
    "    # Scraping data of total salary given on page\n",
    "    Sal_No = driver.find_elements_by_xpath(\"//p[@class='css-1uyte9r css-1kuy7z7 m-0 ']\")\n",
    "    for j in Sal_No:\n",
    "        salary_no.append(j.text)\n",
    "    \n",
    "    # Scraping data of Average Salary\n",
    "    Avg = driver.find_elements_by_xpath(\"//div[@class='col-2 d-none d-md-flex flex-row justify-content-end']\")\n",
    "    for k in Avg:\n",
    "        avg_sal.append(k.text.replace(\"\\n\",\"\"))\n",
    "    \n",
    "    # Scraping data of Min and Max Salary\n",
    "    Min_Sal = driver.find_elements_by_xpath(\"//div[@class='common__RangeBarStyle__values d-flex justify-content-between ']\")\n",
    "    for l in Min_Sal:\n",
    "        min_sal.append(l.text.replace(\"\\n\",\" \"))\n",
    "      \n",
    "       \n",
    "\n",
    "    jobs = pd.DataFrame({}) # Creating dataframe of scraped data\n",
    "    jobs['Company Name'] = comp_name[0:10]\n",
    "    jobs['Total Salaries'] = salary_no[0:10]\n",
    "    jobs['Average Salary'] = avg_sal[0:10]\n",
    "    jobs['Min and Max Salary'] = min_sal[0:10]\n",
    "    \n",
    "    print(jobs)\n",
    "\n",
    "# Calling Function\n",
    "glassdoor_salary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "        1. Brand\n",
    "        2. Product Description\n",
    "        3. Price\n",
    "        4. Discount %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Brand                                        Description Price  \\\n",
      "0       NuVew              UV Protection Aviator Sunglasses (57)  ₹195   \n",
      "1      Aislin   UV Protection, Gradient Wayfarer Sunglasses (57)  ₹625   \n",
      "2    Fastrack      UV Protection Wayfarer Sunglasses (Free Size)  ₹758   \n",
      "3    Fastrack   UV Protection Rectangular Sunglasses (Free Size)  ₹666   \n",
      "4    Fastrack  Mirrored, UV Protection Wayfarer Sunglasses (F...  ₹499   \n",
      "..        ...                                                ...   ...   \n",
      "95  ROYAL SON   UV Protection, Gradient Wayfarer Sunglasses (55)  ₹217   \n",
      "96       hipe         UV Protection Round Sunglasses (Free Size)  ₹209   \n",
      "97     Aislin    UV Protection, Gradient Cat-eye Sunglasses (58)  ₹485   \n",
      "98     Aislin  UV Protection, Gradient Butterfly, Over-sized ...  ₹598   \n",
      "99  ROYAL SON         UV Protection Retro Square Sunglasses (88)  ₹599   \n",
      "\n",
      "   Discount  \n",
      "0   75% off  \n",
      "1   70% off  \n",
      "2   15% off  \n",
      "3   16% off  \n",
      "4   50% off  \n",
      "..      ...  \n",
      "95  78% off  \n",
      "96  83% off  \n",
      "97  68% off  \n",
      "98  72% off  \n",
      "99  70% off  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating open list to save scraped data\n",
    "brand = []\n",
    "desc = []\n",
    "price = []\n",
    "discount = []\n",
    "url_list = []\n",
    "\n",
    "def flipkart_sunglass(): # Defining function flipkart_sunglass\n",
    "    driver = webdriver.Chrome(r\"C://chromedriver.exe\") # loading webdriver\n",
    "    driver.get(\"https://www.flipkart.com/\") # Website from which we have to scrape data\n",
    "    \n",
    "    # Closing login pop-up\n",
    "    pop_btn = driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\").click()\n",
    "    \n",
    "    # finding search job tab and giving input sunglasses\n",
    "    search_job = driver.find_element_by_xpath(\"//input[@type = 'text']\").send_keys(\"sunglasses\") \n",
    "    \n",
    "    # Clicking on Search button\n",
    "    search_btn = driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\").click()\n",
    "        \n",
    "    time.sleep(3)\n",
    "    \n",
    "    # extracting href of products and appending it\n",
    "    url = driver.find_elements_by_xpath(\"//a[@class = 'ge-49M' or @class = 'ge-49M _2Kfbh8']\")\n",
    "    for i in url:\n",
    "        link = i.get_attribute('href')\n",
    "        url_list.append(link)\n",
    "\n",
    "    for x in url_list[0:6]: # opening each href one by one and scraping data\n",
    "        driver.get(x)\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Scraping data of Brand\n",
    "        for j in driver.find_elements_by_xpath(\"//div[@class = '_2WkVRV']\"):\n",
    "            brand.append(j.text)\n",
    "        \n",
    "        # Scraping data of Description\n",
    "        for k in driver.find_elements_by_xpath(\"//a[@class = 'IRpwTa']\"):\n",
    "            desc.append(k.text)\n",
    "        \n",
    "        # Scraping data of Price\n",
    "        for l in driver.find_elements_by_xpath(\"//div[@class = '_30jeq3']\"):\n",
    "            price.append(l.text)\n",
    "        \n",
    "        ## Scraping data of Discount\n",
    "        for m in driver.find_elements_by_xpath(\"//div[@class = '_3Ay6Sb']\"):\n",
    "            discount.append(m.text)\n",
    "    \n",
    "       \n",
    "\n",
    "    sunglasses = pd.DataFrame({}) # Creating dataframe to save scraped data\n",
    "    sunglasses['Brand'] = brand[0:100]\n",
    "    sunglasses['Description'] = desc[0:100]\n",
    "    sunglasses['Price'] = price[0:100]\n",
    "    sunglasses['Discount'] = discount[0:100]\n",
    "    print(sunglasses)\n",
    "    \n",
    "\n",
    "# Calling Function\n",
    "flipkart_sunglass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to \n",
    "go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes\u0002earpods-power\u0002adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCmTSVZAXUHGREPBFGI&marketplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Review_Summary Ratings  \\\n",
      "0            Brilliant       5   \n",
      "1     Perfect product!       5   \n",
      "2    Worth every penny       5   \n",
      "3        Great product       5   \n",
      "4   Highly recommended       5   \n",
      "..                 ...     ...   \n",
      "94   Worth every penny       5   \n",
      "95           Fabulous!       5   \n",
      "96           Wonderful       5   \n",
      "97           Fabulous!       5   \n",
      "98      Classy product       5   \n",
      "\n",
      "                                     Full_Description  \n",
      "0   The Best Phone for the Money  The iPhone 11 of...  \n",
      "1   Amazing phone with great cameras and better ba...  \n",
      "2   Previously I was using one plus 3t it was a gr...  \n",
      "3   Amazing Powerful and Durable Gadget.  I’m am v...  \n",
      "4   iphone 11 is a very good phone to buy only if ...  \n",
      "..                                                ...  \n",
      "94  Just go for it without a second thought, if yo...  \n",
      "95  I really liked the budget iPhone. First I thou...  \n",
      "96  Dont think too much guys. perfect phone for da...  \n",
      "97  This is my first I phone and I'm very happy to...  \n",
      "98  Amazing delivery. Got my phone a day before ex...  \n",
      "\n",
      "[99 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating open list to save scraped data\n",
    "review_summ = []\n",
    "ratings = []\n",
    "full_review = []\n",
    "url_list = []\n",
    "\n",
    "def flipkart_review(): # Defining function flipkart_review\n",
    "    driver = webdriver.Chrome(r\"C://chromedriver.exe\") # loading chrome driver\n",
    "    \n",
    "    # Website link from where we have to scrape data\n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART\")\n",
    "    \n",
    "    # Extracting href of links and appending it to a list\n",
    "    url = driver.find_elements_by_xpath(\"//a[@class = 'ge-49M' or @class = 'ge-49M _2Kfbh8']\")\n",
    "    for a in url[0:11]:\n",
    "        link = a.get_attribute('href')\n",
    "        url_list.append(link)\n",
    "        \n",
    "    # Loop for opening each href to scrape data\n",
    "    for m in url_list:\n",
    "        driver.get(m)\n",
    "        time.sleep(4)\n",
    "        \n",
    "        # Scraping data of review_summary\n",
    "        for i in driver.find_elements_by_xpath(\"//p[@class = '_2-N8zT']\"):\n",
    "            review_summ.append(i.text)\n",
    "        \n",
    "        # Scraping data of Ratings\n",
    "        for j in driver.find_elements_by_xpath(\"//div[@class = '_3LWZlK _1BLPMq']\"):\n",
    "            ratings.append(j.text)\n",
    "        \n",
    "        # Scraping data of full review\n",
    "        for k in driver.find_elements_by_xpath(\"//div[@class = 't-ZTKy']\"):\n",
    "            full_review.append(k.text.replace('\\n',\" \"))\n",
    "        \n",
    "       \n",
    "\n",
    "    flipkart = pd.DataFrame({}) # Saving scraped data into dataframe\n",
    "    flipkart['Review_Summary'] = review_summ[0:99]\n",
    "    flipkart['Ratings'] = ratings[0:99]\n",
    "    flipkart['Full_Description'] = full_review[0:99]\n",
    "    print(flipkart)\n",
    "\n",
    "# Calling Function\n",
    "flipkart_review()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    "    You have to scrape 4 attributes of each sneaker :\n",
    "                1. Brand\n",
    "                2. Product Description\n",
    "                3. Price\n",
    "                4. discount %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Brand                                Product Description  \\\n",
      "0   French Connection                                   Sneakers For Men   \n",
      "1        M K FOOTWEAR  Perfect & Affordable Combo Pack of 02 Pairs Sn...   \n",
      "2              Chevit     Casual Sneakers Shoes For Men Sneakers For Men   \n",
      "3        Robbie jones  Combo Pack of 4 Casual Sneakers With Sneakers ...   \n",
      "4              Chevit                                   Sneakers For Men   \n",
      "..                ...                                                ...   \n",
      "95              Birde                White Casual Shoes Sneakers For Men   \n",
      "96           Magnolia            Combo Pack Of 4 Casual Sneakers For Men   \n",
      "97          ROCKFIELD  Combo Pack of 2 Latest Collection Stylish Casu...   \n",
      "98              Creer  Casual Loafers, Sneakers Shoes for Men Pack of...   \n",
      "99  French Connection  Rockstyle Trending Multicolor Ultralight canva...   \n",
      "\n",
      "   Discount Price  \n",
      "0   60% off  ₹799  \n",
      "1   65% off  ₹349  \n",
      "2   72% off  ₹499  \n",
      "3   62% off  ₹379  \n",
      "4   76% off  ₹474  \n",
      "..      ...   ...  \n",
      "95  60% off  ₹599  \n",
      "96  62% off  ₹377  \n",
      "97  60% off  ₹399  \n",
      "98  60% off  ₹398  \n",
      "99  60% off  ₹799  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating open list to save scraped data\n",
    "brand = []\n",
    "desc = []\n",
    "price = []\n",
    "discount = []\n",
    "url_list = []\n",
    "\n",
    "def flipkart_sneakers(): # Defining function\n",
    "    driver = webdriver.Chrome(r\"C://chromedriver.exe\")\n",
    "    \n",
    "    # Url from where we will scrape data\n",
    "    driver.get(\"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\")\n",
    "    \n",
    "    # extracting href data and appending it\n",
    "    url = driver.find_elements_by_xpath(\"//a[@class = 'ge-49M' or @class = 'ge-49M _2Kfbh8']\")\n",
    "    for a in url[0:11]:\n",
    "        link = a.get_attribute('href')\n",
    "        url_list.append(link)\n",
    "    \n",
    "    # opening each href to scrape data\n",
    "    for m in url_list:\n",
    "        driver.get(m)\n",
    "        time.sleep(4)\n",
    "        \n",
    "       # Scraping data of Brand \n",
    "        for i in driver.find_elements_by_xpath(\"//div[@class = '_2WkVRV']\"):\n",
    "            brand.append(i.text)\n",
    "        \n",
    "        # Scraping data of description\n",
    "        for j in driver.find_elements_by_xpath(\"//a[@class = 'IRpwTa']\"):\n",
    "            desc.append(j.text)\n",
    "        \n",
    "        # Scraping data of Price\n",
    "        for k in driver.find_elements_by_xpath(\"//div[@class = '_30jeq3']\"):\n",
    "            price.append(k.text)\n",
    "        \n",
    "        # Scraping data of Discount\n",
    "        for l in driver.find_elements_by_xpath(\"//div[@class = '_3Ay6Sb']\"):\n",
    "            discount.append(l.text)\n",
    "        \n",
    "       \n",
    "\n",
    "    flipkart = pd.DataFrame({})\n",
    "    flipkart['Brand'] = brand[0:100]\n",
    "    flipkart['Product Description'] = desc[0:100]\n",
    "    flipkart['Discount'] = discount[0:100]\n",
    "    flipkart['Price'] = price[0:100]\n",
    "    print(flipkart)\n",
    "\n",
    "# Calling Function\n",
    "flipkart_sneakers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9: Go to the link - https://www.myntra.com/shoes. Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”,\n",
    "    then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, \n",
    "    price of the shoe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Brand                 Product Description      Price\n",
      "0                   Nike          Men AIR ZOOM Running Shoes  Rs. 10796\n",
      "1                   Nike         Men JORDAN DELTA Basketball  Rs. 13495\n",
      "2                   Nike                  Men AF1/1 Sneakers  Rs. 10620\n",
      "3        PUMA Motorsport       Unisex Mercedes Running Shoes  Rs. 12495\n",
      "4                   Nike      AIR ZOOM PEGASUS Running Shoes   Rs. 7496\n",
      "..                   ...                                 ...        ...\n",
      "95              DAVINCHI                        Ustraa black  Rs. 10295\n",
      "96                  Nike         Women AIR MAX VIVA Sneakers   Rs. 9996\n",
      "97              DAVINCHI         Men Formal Leather Slip-Ons  Rs. 12495\n",
      "98  Heel & Buckle London  Men Textured Leather Penny Loafers   Rs. 7796\n",
      "99          UNDER ARMOUR          HOVR Sonic 3 Running Shoes  Rs. 11995\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating open list to save the scraped data\n",
    "brand = []\n",
    "desc = []\n",
    "price = []\n",
    "\n",
    "\n",
    "def myntra_shoes(): # defining function myntra_shoes\n",
    "    driver = webdriver.Chrome(\"C://chromedriver.exe\") # loading chrome driver\n",
    "    driver.get(\"https://www.myntra.com/shoes\") # website from which we will scrape data\n",
    "    \n",
    "    # Applying color filter\n",
    "    color_filter = driver.find_element_by_xpath(\"//li[@class = 'colour-listItem']\").click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Applying price filter\n",
    "    price_filter = driver.find_element_by_xpath(\"//ul[@class='price-list']/li[2]/label\").click()\n",
    "    \n",
    "    time.sleep(3)\n",
    "    for a in range(0,5): # loop for extracting data.\n",
    "        \n",
    "        # Scraping data of Brand\n",
    "        for i in driver.find_elements_by_xpath(\"//h3[@class = 'product-brand']\"):\n",
    "            brand.append(i.text)\n",
    "        \n",
    "        # Scraping data Description\n",
    "        for j in driver.find_elements_by_xpath(\"//h4[@class = 'product-product']\"):\n",
    "            desc.append(j.text)\n",
    "        \n",
    "        # Scraping data of Price\n",
    "        for k in driver.find_elements_by_xpath(\"//div[@class = 'product-price']/span/span\"):\n",
    "            price.append(k.text)\n",
    "    \n",
    "    # Link for automatic proceeding to next page to scrape data\n",
    "    next_url = driver.find_element_by_xpath(\"//li[@class = 'pagination-next']/a\").get_attribute(\"href\")\n",
    "    driver.get(next_url)\n",
    "        \n",
    "       \n",
    "\n",
    "    myntra_shoe = pd.DataFrame({}) # saving scraped data into dataframe\n",
    "    myntra_shoe['Brand'] = brand[0:100]\n",
    "    myntra_shoe['Product Description'] = desc[0:100]\n",
    "    myntra_shoe['Price'] = price[0:100]\n",
    "    print(myntra_shoe)\n",
    "\n",
    "# Calling Function\n",
    "myntra_shoes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon.\n",
    "     Then set CPU Type filter to “Intel Core i7” and “Intel Core i9”.After setting the filters scrape first 10 laptops data.\n",
    "     You have to scrape 3 attributes for each laptop:\n",
    "            1. title\n",
    "            2. Ratings\n",
    "            3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title     L-Ratings     Price\n",
      "0  Lenovo Yoga 9 11th Gen Intel Core i7 14-inch 4...    5 out of 5  1,67,990\n",
      "1  (Renewed) HP ZBook 15 G3 Mobile Workstation - ...    No Ratings    83,990\n",
      "2  HP 14 Thin & Light 14-inch FHD Laptop (11th Ge...  4.6 out of 5    76,500\n",
      "3  Mi Notebook Horizon Edition 14 Intel Core i5-1...  4.3 out of 5    54,999\n",
      "4  Dell Alienware m15(R3) 15.6-inch FHD Gaming La...  2.7 out of 5  1,98,590\n",
      "5  Lenovo Legion 5Pi 10th Gen Intel Core i7 15.6\"...  4.3 out of 5  1,35,490\n",
      "6  Asus ROG Zephyrus S Ultra Slim Gaming Laptop, ...  4.3 out of 5  2,59,990\n",
      "7  ASUS ZenBook Pro Duo Intel Core i9-10980HK 10t...  3.3 out of 5  1,20,800\n",
      "8  Lenovo Yoga S740 Intel Core i7 10th Gen 14 inc...  3.3 out of 5    78,990\n",
      "9  Lenovo Legion Y540 Intel Core i7 9th Gen 15.6 ...  4.1 out of 5  2,27,200\n"
     ]
    }
   ],
   "source": [
    "# Creating open list to save scrape results\n",
    "title = []\n",
    "ratings = []\n",
    "price = []\n",
    "link = []\n",
    "\n",
    "def Amazon_Laptop():  #Creating function\n",
    "    driver = webdriver.Chrome(\"C://chromedriver.exe\") # Loading chrome driver\n",
    "    driver.get(\"https://www.amazon.in/\") # website from which we will scrape data\n",
    "    \n",
    "    # Finding search input and giving input\n",
    "    search = driver.find_element_by_id(\"twotabsearchtextbox\").send_keys(\"Laptop\")\n",
    "    \n",
    "    # clicking on search button\n",
    "    search_btn = driver.find_element_by_xpath(\"//div[@class='nav-search-submit nav-sprite']\").click()\n",
    "    \n",
    "    # applying i7 filter\n",
    "    i7_filter = driver.find_elements_by_xpath(\"//a[@class = 'a-link-normal s-navigation-item']/span\")\n",
    "    for i in i7_filter:\n",
    "        if i.text == 'Intel Core i7':\n",
    "            i.click()\n",
    "            break\n",
    "    \n",
    "    # applying i9 data\n",
    "    i9_filter = driver.find_elements_by_xpath(\"//a[@class = 'a-link-normal s-navigation-item']/span\")\n",
    "    for j in i9_filter:\n",
    "        if j.text == 'Intel Core i9':\n",
    "            j.click()\n",
    "            break\n",
    "      \n",
    "    # Scraping data of title\n",
    "    Title = driver.find_elements_by_xpath(\"//span[@class = 'a-size-medium a-color-base a-text-normal']\")\n",
    "    for k in Title[0:10]:\n",
    "        title.append(k.text)\n",
    "    \n",
    "    # Scraping data of price\n",
    "    Price = driver.find_elements_by_xpath(\"//span[@class = 'a-price-whole']\")\n",
    "    for l in Price[0:10]:\n",
    "        price.append(l.text)\n",
    "    \n",
    "    # Scraping data of ratings\n",
    "    url = driver.find_elements_by_xpath(\"//a[@class = 'a-link-normal a-text-normal']\")\n",
    "    for d in url[0:10]:\n",
    "        link.append(d.get_attribute('href'))\n",
    "    for e in link:\n",
    "        driver.get(e)\n",
    "        try:\n",
    "            rate = driver.find_element_by_xpath(\"//span[@id ='acrCustomerReviewText']\")\n",
    "            rate.click()\n",
    "            Ratings = driver.find_element_by_xpath(\"//span[@class='a-size-medium a-color-base']\")\n",
    "            ratings.append(Ratings.text)\n",
    "        except NoSuchElementException as e:\n",
    "            ratings.append(\"No Ratings\")     \n",
    "       \n",
    "\n",
    "    laptops = pd.DataFrame({}) # saving scraped data into dataframe.\n",
    "    laptops['Title'] = title[0:10]\n",
    "    laptops['L-Ratings'] = ratings[0:10]\n",
    "    laptops['Price'] = price[0:10]\n",
    "    print(laptops)\n",
    "\n",
    "# Calling Function\n",
    "Amazon_Laptop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EOD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
